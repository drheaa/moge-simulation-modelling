%=========================================================
% CHAPTER 5: PARAMETER ESTIMATION
%=========================================================

\chapter{Parameter Estimation}
\label{chap:estimation}

In this chapter, we derive the maximum likelihood estimators (MLEs) of the unknown
parameters of the Marshall--Olkin Generalized Exponential (MOGE) distribution.
We begin by formulating the observed-data log-likelihood and associated score
equations. We then construct the complete-data likelihood by introducing a latent
variable $Z$, which enables the implementation of an EM algorithm. This chapter
focuses on the estimation procedure rather than theoretical properties of the MLEs.


%=========================================================
\section{Observed Log-Likelihood Function}
%=========================================================

Let $X_1,\ldots,X_n$ be a sample from the 
$\mathrm{MOGE}(\alpha,\lambda,\theta)$ distribution.
The observed-data log-likelihood is

\begin{equation}
\label{eq:loglik}
\ell(\alpha,\lambda,\theta)
=
n \log(\alpha\lambda\theta)
-
\lambda \sum_{i=1}^n x_i
+
(\alpha - 1)\sum_{i=1}^n \log(1 - e^{-\lambda x_i})
-
2\sum_{i=1}^n
\log\!\left(
\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha
\right).
\end{equation}

\noindent\textbf{Remark.}
The final term reflects the Marshall--Olkin shock mechanism and is
responsible for the nonlinear dependence on $\theta$ and $\alpha$.


%=========================================================
\section{Score Equations}
%=========================================================

The score equations follow from differentiating \eqref{eq:loglik} with respect
to $(\lambda,\alpha,\theta)$.

%---------------------------------------------------------
\subsection{Derivative with respect to $\lambda$}
%---------------------------------------------------------

\begin{align}
\frac{\partial \ell}{\partial \lambda}
&=
\frac{n}{\lambda}
-
\sum_{i=1}^n x_i
+
(\alpha - 1)
\sum_{i=1}^n
\frac{x_i e^{-\lambda x_i}}{1 - e^{-\lambda x_i}}
\nonumber \\
&\quad
-
2(1-\theta)\alpha
\sum_{i=1}^n
\frac{
x_i e^{-\lambda x_i}(1-e^{-\lambda x_i})^{\alpha - 1}
}{
\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha
}.
\label{eq:score_lambda}
\end{align}

%---------------------------------------------------------
\subsection{Derivative with respect to $\alpha$}
%---------------------------------------------------------

\begin{align}
\frac{\partial \ell}{\partial \alpha}
=
\frac{n}{\alpha}
+
\sum_{i=1}^n \log(1-e^{-\lambda x_i})
-
2(1-\theta)
\sum_{i=1}^n
\frac{
(1-e^{-\lambda x_i})^\alpha
\log(1-e^{-\lambda x_i})
}{
\theta+(1-\theta)(1-e^{-\lambda x_i})^\alpha
}.
\label{eq:score_alpha}
\end{align}

%---------------------------------------------------------
\subsection{Derivative with respect to $\theta$}
%---------------------------------------------------------

\begin{equation}
\frac{\partial \ell}{\partial \theta}
=
\frac{n}{\theta}
-
2\sum_{i=1}^n
\frac{
1 - (1-e^{-\lambda x_i})^\alpha
}{
\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha
}.
\label{eq:score_theta}
\end{equation}

\noindent\textbf{Remark.}
These equations do not yield closed-form solutions, motivating the EM
algorithm developed later in the chapter.


%=========================================================
\section{Complete-Data Likelihood and Latent Structure}
%=========================================================

To enable EM estimation, the MOGE model is augmented with a latent variable $Z$,
leading to the joint density

\begin{equation}
\label{eq:jointpdf}
f(x,z;\alpha,\lambda,\theta)
=
\frac{
\alpha\lambda\theta\,
e^{-\lambda x}(1-e^{-\lambda x})^{\alpha - 1}
}{
(1-(1-e^{-\lambda x})^\alpha)^2
}
\exp\!\left[
-z\Bigl(\theta - 1 + (1-(1-e^{-\lambda x})^\alpha)^{-1}\Bigr)
\right].
\end{equation}

The corresponding complete-data log-likelihood is

\begin{align}
\ell_c(\alpha,\lambda,\theta)
&=
n\log\alpha
+
n\log\lambda
+
n\log\theta
-
\lambda\sum_{i=1}^n x_i
+
(\alpha - 1)\sum_{i=1}^n\log(1-e^{-\lambda x_i})
\nonumber\\
&\quad
-2\sum_{i=1}^n\log\!\left(1-(1-e^{-\lambda x_i})^\alpha\right)
-\sum_{i=1}^n
z_i\left[\theta - 1 + (1-(1-e^{-\lambda x_i})^\alpha)^{-1}\right].
\label{eq:complete_ll}
\end{align}

\noindent\textbf{Key observation.}
Maximization with respect to $\theta$ separates cleanly:
\[
\frac{\partial \ell_c}{\partial\theta}
=
\frac{n}{\theta}
-
\sum_{i=1}^n z_i
\quad\Rightarrow\quad
\hat{\theta}
=
\frac{n}{\sum_{i=1}^n z_i}.
\]


%=========================================================
\section{Decomposition into \texorpdfstring{$g_1$}{g1} and \texorpdfstring{$g_2$}{g2}}
%=========================================================

Define the function
\[
g(\alpha,\lambda) = g_1(\alpha,\lambda) + g_2(\alpha,\lambda),
\]
where

\begin{align}
g_1(\alpha,\lambda)
&=
n\ln\alpha
+
n\ln\lambda
-
\lambda\sum_{i=1}^n x_i
+
(\alpha - 1)\sum_{i=1}^n\log(1-e^{-\lambda x_i}),
\\[4pt]
g_2(\alpha,\lambda)
&=
-2\sum_{i=1}^n\log\!\bigl(1-(1-e^{-\lambda x_i})^\alpha\bigr)
-
\sum_{i=1}^n z_i(1-(1-e^{-\lambda x_i})^\alpha)^{-1}.
\end{align}

This decomposition allows fixed-point updates for $(\alpha,\lambda)$.


%=========================================================
\section{Fixed-Point Optimization for $(\alpha,\lambda)$}
%=========================================================

The system to be solved at iteration $m$ is
\[
g_1'(\alpha,\lambda)
=
-g_2'(\alpha^{(m)},\lambda^{(m)}).
\]

%---------------------------------------------------------
\subsection{Initial Fixed-Point Updates}
%---------------------------------------------------------

Solving $g_{1,\lambda} = 0$ yields the fixed-point update

\begin{equation}
\boxed{
\lambda
=
\left[
\frac{1}{n}\sum_{i=1}^n \frac{x_i e^{-\lambda x_i}}{1-e^{-\lambda x_i}}
\left(1+\frac{n}{\sum_{j=1}^n \log(1-e^{-\lambda x_j})}\right)
+
\frac{1}{n}\sum_{i=1}^n x_i
\right]^{-1}
}
\end{equation}

with corresponding update

\begin{equation}
\boxed{
\alpha
=
-\frac{n}{\sum_{i=1}^n\log(1-e^{-\lambda x_i})}.
}
\end{equation}


%---------------------------------------------------------
\subsection{General Fixed-Point Updates}
%---------------------------------------------------------

Let
\[
c_1 = -g_{2,\alpha}(\alpha^{(m)},\lambda^{(m)}),
\qquad
c_2 = -g_{2,\lambda}(\alpha^{(m)},\lambda^{(m)}).
\]

Then the updated values satisfy

\begin{equation}
\boxed{
\lambda
=
\left[
\frac{c_2}{n}
+
\frac{1}{n}\sum_{i=1}^n x_i
+
\left(
1-\frac{n}{c_1 - \sum_{i=1}^n\log(1-e^{-\lambda x_i})}
\right)
\left(
\frac{1}{n}\sum_{i=1}^n
\frac{x_i e^{-\lambda x_i}}{1-e^{-\lambda x_i}}
\right)
\right]^{-1}
}
\end{equation}

\begin{equation}
\boxed{
\alpha
=
\left[
\frac{
c_1 - \sum_{i=1}^n\log(1-e^{-\lambda x_i})
}{n}
\right]^{-1}.
}
\end{equation}


%=========================================================
\section{EM Algorithm for the MOGE Model}
%=========================================================

The conditional expectation required in the E-step is

\begin{equation}
\boxed{
E(Z\mid X=x;\alpha,\lambda,\theta)
=
\frac{
2\left(1-(1-e^{-\lambda x})^\alpha\right)
}{
\theta + (1-\theta)(1-e^{-\lambda x})^\alpha
}
}
\end{equation}

Let
\[
z_i^{(k)} = E(Z\mid X=x_i;\alpha^{(k)},\lambda^{(k)},\theta^{(k)}).
\]

%---------------------------------------------------------
\subsection{E-step}
%---------------------------------------------------------

Compute all $z_i^{(k)}$ and substitute them into the complete-data
log-likelihood \eqref{eq:complete_ll}.

%---------------------------------------------------------
\subsection{M-step}
%---------------------------------------------------------

\[
\theta^{(k+1)} = \frac{n}{\sum_{i=1}^n z_i^{(k)}}.
\]

Compute $c_1, c_2$ from $g_2$ at $(\alpha^{(k)},\lambda^{(k)})$ and update
$(\alpha,\lambda)$ using the fixed-point equations above.

Iterate until convergence.


%=========================================================
\section{Summary}
%=========================================================

This chapter developed the complete likelihood-based estimation procedure for the
MOGE distribution. The EM algorithm avoids full three-dimensional optimization by
separating the update for $\theta$ and applying fixed-point iterations for $(\alpha,\lambda)$.
The resulting method is efficient, stable, and well suited for numerical implementation.