%=========================================================
% CHAPTER 5: PARAMETER ESTIMATION
%=========================================================


\chapter{Parameter Estimation}
\label{chap:estimation}


In this chapter, we derive the maximum likelihood estimators (MLEs) of the unknown
parameters of the Marshall--Olkin Generalized Exponential (MOGE) distribution.
We first develop the observed-data log-likelihood and compute the corresponding
score equations. Next, we establish theoretical properties of the MLE of the
shape parameter $\alpha$. We then introduce the complete-data formulation
with the latent variable $Z$ and derive the EM algorithm following the
framework presented in the original papers by Risti\'{c} and Kundu (2015)
and Song, Fan and Kalbfleisch (2005). Short explanatory remarks are included
to clarify the main steps.


%=========================================================
\section{Observed Log-Likelihood Function}
%=========================================================


Let $X_1, X_2, \ldots, X_n$ be a complete sample from the
$\mathrm{MOGE}(\alpha,\lambda,\theta)$ distribution.
The observed-data log-likelihood is


\begin{equation}
\label{eq:loglik}
\ell(\alpha,\lambda,\theta)
=
n \log(\alpha\lambda\theta)
-
\lambda \sum_{i=1}^n x_i
+
(\alpha-1) \sum_{i=1}^n \log(1-e^{-\lambda x_i})
-
2 \sum_{i=1}^n
\log\!\bigl(\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha \bigr).
\end{equation}


\noindent
\textbf{Remark.}
This expression is obtained by applying $\log(\cdot)$ to the MOGE density
and summing term-wise over the sample. The last term arises from the
Marshall--Olkin shock-formation structure.


%=========================================================
\section{Score Equations}
%=========================================================


The score equations are obtained by setting
$
\partial \ell/\partial \lambda = 0,\,
\partial \ell/\partial \alpha = 0,\,
\partial \ell/\partial \theta = 0.
$


%---------------------------------------------------------
\subsection{Derivative with respect to $\lambda$}
%---------------------------------------------------------


\begin{align}
\frac{\partial \ell}{\partial \lambda}
&=
\frac{n}{\lambda}
-
\sum_{i=1}^n x_i
+
(\alpha - 1)
\sum_{i=1}^n
\frac{x_i e^{-\lambda x_i}}{1 - e^{-\lambda x_i}}
\nonumber \\
&\quad
-
2(1-\theta)\alpha
\sum_{i=1}^n
\frac{
x_i e^{-\lambda x_i}
\,(1-e^{-\lambda x_i})^{\alpha-1}
}{
\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha
}.
\label{eq:score_lambda}
\end{align}


\noindent
\textbf{Remark.}
The first two terms come from differentiating
$n\log\lambda - \lambda \sum x_i$, while the last two follow from
the chain rule applied to the terms involving $\log(1 - e^{-\lambda x_i})$
and $\log(\theta+(1-\theta)A_i^\alpha)$.


%---------------------------------------------------------
\subsection{Derivative with respect to $\alpha$}
%---------------------------------------------------------


\begin{align}
\frac{\partial \ell}{\partial \alpha}
&=
\frac{n}{\alpha}
+
\sum_{i=1}^n \log(1-e^{-\lambda x_i})
-
2(1-\theta)
\sum_{i=1}^n
\frac{
(1-e^{-\lambda x_i})^{\alpha} \log(1-e^{-\lambda x_i})
}{
\theta + (1-\theta)(1-e^{-\lambda x_i})^{\alpha}
}.
\label{eq:score_alpha}
\end{align}


%---------------------------------------------------------
\subsection{Derivative with respect to $\theta$}
%---------------------------------------------------------


\begin{equation}
\frac{\partial \ell}{\partial \theta}
=
\frac{n}{\theta}
-
2 \sum_{i=1}^n
\frac{
1 - (1-e^{-\lambda x_i})^\alpha
}{
\theta + (1-\theta)(1-e^{-\lambda x_i})^\alpha
}.
\label{eq:score_theta}
\end{equation}


\noindent
\textbf{Remark.}
The dependence on $\theta$ appears only through the final term of the
log-likelihood, hence the simplified form of~\eqref{eq:score_theta}.


%=========================================================
\section{Properties of the MLE of $\alpha$}
%=========================================================


Let
\[
\psi = -\frac{1}{n} \sum_{i=1}^n \log(1-e^{-\lambda x_i}) > 0.
\]


We restate the result of Risti\'{c} and Kundu (2015):


\begin{theorem}
    \label{thm:alpha_root}
    Let $\alpha$ denote the true parameter.
    If $0 < \theta < 1$, then the equation
    \[
    \frac{\partial \ell}{\partial \alpha} = 0
    \]
    has exactly one solution.
    If $\theta > 1$, then the solution lies in the interval
    \[
    \bigl[(2\theta - 1)^{-1} \psi^{-1},\; \psi^{-1} \bigr].
    \]
    \end{theorem}
   
    \begin{proof}
    Omitted for brevity; the proof follows by analyzing the monotonicity of the
    score function~\eqref{eq:score_alpha} and applying boundary limits
    as $\alpha \to 0^{+}$ and $\alpha \to \infty$.
    \end{proof}
   
    \noindent\textbf{Remark.}
    This ensures numerical stability when solving for $\alpha$ in the EM algorithm.
   


%=========================================================
\section{Complete-Data Likelihood and Latent Variable Structure}
%=========================================================


The core idea behind the EM algorithm is to augment the sample
with an unobservable variable $Z$. The joint pdf of $(X,Z)$ is


\begin{equation}
\label{eq:jointpdf}
f(x,z;\alpha,\lambda,\theta)
=
\frac{\alpha \lambda \theta e^{-\lambda x}(1-e^{-\lambda x})^{\alpha-1}}
{
\left(1-(1-e^{-\lambda x})^\alpha\right)^2
}
\,
\exp\!\left[
-z\left(
\theta - 1 + (1-(1-e^{-\lambda x})^\alpha)^{-1}
\right)
\right].
\tag{7}
\end{equation}


Summing the log of~\eqref{eq:jointpdf} over $i=1,\ldots,n$ gives
the complete-data log-likelihood:


\begin{align}
\ell(\alpha,\lambda,\theta; \{x_i,z_i\})
&=
n\log\alpha
+
n\log\lambda
+
n\log\theta
-
\lambda \sum_{i=1}^n x_i
+
(\alpha-1)\sum_{i=1}^n \log(1-e^{-\lambda x_i})
\nonumber\\
&\quad
-2\sum_{i=1}^n\log\left(1-(1-e^{-\lambda x_i})^\alpha\right)
-\sum_{i=1}^n z_i\left[
\theta - 1 + (1-(1-e^{-\lambda x_i})^\alpha)^{-1}
\right].
\tag{8}
\end{align}


\noindent
\textbf{Remark.}  
Maximization with respect to $\theta$ separates cleanly:
\[
\frac{\partial \ell}{\partial\theta}
=
\frac{n}{\theta}
-
\sum_{i=1}^n z_i,
\quad
\Rightarrow\quad
\hat\theta = \frac{n}{\sum z_i}.
\]


%=========================================================
\section{Decomposition into \texorpdfstring{$g_1$}{g1} and \texorpdfstring{$g_2$}{g2}}
%=========================================================


Define
\[
g(\alpha,\lambda)
=
g_1(\alpha,\lambda) + g_2(\alpha,\lambda),
\tag{10}
\]
where


\begin{align}
g_1(\alpha,\lambda)
&=
n\ln\alpha
+
n\ln\lambda
-
\lambda \sum_{i=1}^n x_i
+
(\alpha-1)\sum_{i=1}^n \log(1-e^{-\lambda x_i}),
\tag{11}
\\[6pt]
g_2(\alpha,\lambda)
&=
-2\sum_{i=1}^n
\log\left(1-(1-e^{-\lambda x_i})^\alpha\right)
-
\sum_{i=1}^n z_i (1-(1-e^{-\lambda x_i})^\alpha)^{-1}.
\tag{12}
\end{align}


%=========================================================
\section{Fixed-Point Optimization for \texorpdfstring{$(\alpha,\lambda)$}{(alpha,lambda)}}
%=========================================================


We solve the system
\[
g'_1(\alpha,\lambda)
=
-g'_2(\alpha^{(m)},\lambda^{(m)}).
\tag{13}
\]


%---------------------------------------------------------
\subsection{Initial Fixed-Point Equation}
%---------------------------------------------------------


Solving $g_{1,\lambda}=0$ yields the iterative scheme
\begin{equation}
\boxed{
\lambda
=
\left[
\frac{1}{n}\sum_{i=1}^n \frac{x_i e^{-\lambda x_i}}{1-e^{-\lambda x_i}}
\left(1+\frac{n}{\sum_{j=1}^n\log(1-e^{-\lambda x_j})}\right)
+
\frac{1}{n}\sum_{i=1}^n x_i
\right]^{-1}
}
\tag{15}
\end{equation}


and the corresponding update for $\alpha$:
\begin{equation}
\boxed{
\alpha
=
-\frac{n}{\sum_{i=1}^n\log(1-e^{-\lambda x_i})}.
}
\tag{16}
\end{equation}


%---------------------------------------------------------
\subsection{General Fixed-Point Update}
%---------------------------------------------------------


Define constants
\[
c_1 = -g_{2,\alpha}(\alpha^{(m)},\lambda^{(m)}),
\qquad
c_2 = -g_{2,\lambda}(\alpha^{(m)},\lambda^{(m)}).
\]


Then the general update equations (solving $g_{1}' = (c_1,c_2)$) are:


\begin{equation}
\boxed{
\lambda
=
\left[
\frac{c_2}{n}
+
\frac{1}{n}\sum_{i=1}^n x_i
+
\left(
1-\frac{n}{c_1 - \sum_{i=1}^n\log(1-e^{-\lambda x_i})}
\right)
\left(
\frac{1}{n}\sum_{i=1}^n \frac{x_i e^{-\lambda x_i}}{1-e^{-\lambda x_i}}
\right)
\right]^{-1}
}
\tag{20}
\end{equation}


\begin{equation}
\boxed{
\alpha
=
\left[
\frac{
c_1 - \sum_{i=1}^n\log(1-e^{-\lambda x_i})
}{n}
\right]^{-1}.
}
\tag{21}
\end{equation}


%=========================================================
\section{EM Algorithm for the MOGE Model}
%=========================================================


The E-step requires the conditional expectation:


\begin{equation}
\boxed{
E(Z\mid X=x;\alpha,\lambda,\theta)
=
\frac{
2\left(1-(1-e^{-\lambda x})^\alpha\right)
}{
\theta + (1-\theta)(1-e^{-\lambda x})^\alpha
}
}
\tag{22}
\end{equation}


Let
\[
z_i^{(k)}
=
E(Z\mid X=x_i;\alpha^{(k)},\lambda^{(k)},\theta^{(k)}),
\tag{23}
\]


which is substituted into the complete-data log-likelihood.


%---------------------------------------------------------
\subsection{E-step}
%---------------------------------------------------------


Compute
\[
z_i^{(k)}
=
E(Z\mid x_i;\alpha^{(k)},\lambda^{(k)},\theta^{(k)}).
\]


Replace $z_i$ in~\eqref{eq:jointpdf} and~(8) by $z_i^{(k)}$.


%---------------------------------------------------------
\subsection{M-step}
%---------------------------------------------------------


Update:


\[
\theta^{(k+1)}
=
\frac{n}{\sum_{i=1}^n z_i^{(k)}}.
\]


Compute $c_1$ and $c_2$ from $g_2$ at
$(\alpha^{(k)},\lambda^{(k)})$,
then update $(\alpha,\lambda)$ using (20)â€“(21).


Iterate until convergence.


%=========================================================
\section{Summary}
%=========================================================


This chapter developed the complete maximum likelihood estimation framework
for the MOGE distribution, beginning with the observed-data likelihood,
then establishing the mathematical properties of the score equations,
followed by the derivation of the complete-data likelihood and EM algorithm.
The resulting parameter estimation algorithm is efficient and avoids a
full three-dimensional numerical optimization.





